{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a526ee64",
   "metadata": {},
   "source": [
    "# Preprocessing for Docling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e1f3d0",
   "metadata": {},
   "source": [
    "Docling fails at parsing large files (because Docling has timeout set for each of its tasks). Therefore Docling needs the PDFs to be split and having it compressed is an extra improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89377e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ghostscript\n",
    "import sys\n",
    "import os\n",
    "import fitz\n",
    "import httpx\n",
    "from typing import List, Dict\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f8d080",
   "metadata": {},
   "source": [
    "Compression function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a680897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_pdf(input_path, output_path, quality=\"screen\"):\n",
    "    \"\"\"\n",
    "    Compress a PDF using the Ghostscript Python SDK (safe for latest versions).\n",
    "    \"\"\"\n",
    "    quality_settings = {\n",
    "        \"screen\": \"/screen\",\n",
    "        \"ebook\": \"/ebook\",\n",
    "        \"printer\": \"/printer\",\n",
    "        \"prepress\": \"/prepress\",\n",
    "        \"default\": \"/default\"\n",
    "    }\n",
    "\n",
    "    if quality not in quality_settings:\n",
    "        raise ValueError(f\"Invalid quality setting: {quality}\")\n",
    "\n",
    "    # Double check file exists\n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(f\"Input file not found: {input_path}\")\n",
    "\n",
    "    args = [\n",
    "        \"gs\",  # Just a label; not actually used\n",
    "        \"-sDEVICE=pdfwrite\",\n",
    "        \"-dCompatibilityLevel=1.4\",\n",
    "        f\"-dPDFSETTINGS={quality_settings[quality]}\",\n",
    "        \"-dNOPAUSE\",\n",
    "        \"-dQUIET\",\n",
    "        \"-dBATCH\",\n",
    "        f\"-sOutputFile={output_path}\",\n",
    "        input_path\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # Pass plain strings (no need to encode)\n",
    "        with ghostscript.Ghostscript(*args):\n",
    "            pass\n",
    "        print(f\"âœ… Compressed PDF saved to: {output_path}\")\n",
    "        output_size = os.path.getsize(output_path)\n",
    "        input_size = os.path.getsize(input_path)\n",
    "        reduction = 100 * (1 - output_size / input_size)\n",
    "        print(f\"âœ… Compressed PDF saved to: {output_path}\")\n",
    "        print(f\"ðŸ“¦ Original size: {input_size / 1024:.2f} KB\")\n",
    "        print(f\"ðŸ“‰ Compressed size: {output_size / 1024:.2f} KB\")\n",
    "        print(f\"ðŸ§® Compression: {reduction:.2f}%\")\n",
    "    except ghostscript.GhostscriptError as e:\n",
    "        print(f\"âŒ Ghostscript compression failed: {e}\")\n",
    "    except Exception as ex:\n",
    "        print(f\"âŒ Unexpected error: {ex}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784ac92e",
   "metadata": {},
   "source": [
    "Parse Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72a5ccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def docling_parse(input_path: str) -> Dict:\n",
    "    url = \"http://localhost:5001/v1alpha/convert/file\"\n",
    "    parameters = {\n",
    "        \"from_formats\": [\"docx\", \"pptx\", \"html\", \"image\", \"pdf\", \"asciidoc\", \"md\", \"xlsx\"],\n",
    "        \"to_formats\": [\"md\", \"json\", \"html\", \"text\", \"doctags\"],\n",
    "        \"image_export_mode\": \"placeholder\",\n",
    "        \"do_ocr\": True,\n",
    "        \"force_ocr\": False,\n",
    "        \"ocr_engine\": \"easyocr\",\n",
    "        \"ocr_lang\": [\"en\"],\n",
    "        \"pdf_backend\": \"dlparse_v2\",\n",
    "        \"table_mode\": \"fast\",\n",
    "        \"abort_on_error\": False,\n",
    "        \"return_as_file\": False\n",
    "    }\n",
    "\n",
    "    async with httpx.AsyncClient(timeout=60.0) as client:\n",
    "        with open(input_path, \"rb\") as f:\n",
    "            files = {'files': (os.path.basename(input_path), f, 'application/pdf')}\n",
    "            response = await client.post(url, files=files, data=parameters)\n",
    "\n",
    "    response.raise_for_status()\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776c0cf0",
   "metadata": {},
   "source": [
    "Split Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcbc5ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_pdf_into_chunks(input_path: str, output_dir: str, chunk_size: int = 20) -> List[Dict]:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    doc = fitz.open(input_path)\n",
    "    base = os.path.splitext(os.path.basename(input_path))[0]\n",
    "\n",
    "    chunks = []\n",
    "    for start in range(0, len(doc), chunk_size):\n",
    "        end = min(start + chunk_size, len(doc))\n",
    "        chunk_doc = fitz.open()\n",
    "        chunk_doc.insert_pdf(doc, from_page=start, to_page=end - 1)\n",
    "        out_path = os.path.join(output_dir, f\"{base}_pages_{start+1}_to_{end}.pdf\")\n",
    "        chunk_doc.save(out_path)\n",
    "        chunks.append({\n",
    "            \"start_page\": start + 1,\n",
    "            \"end_page\": end,\n",
    "            \"file_path\": out_path\n",
    "        })\n",
    "    print(f\"âœ… Split into {len(chunks)} chunks\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cabbe40",
   "metadata": {},
   "source": [
    "Function to link the chunks to the docling function while preserving page numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11becde",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_all_pages(chunks, output_jsonl_path=\"output.jsonl\"):\n",
    "    with open(output_jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for chunk in tqdm(chunks, desc=\"Parsing chunks\"):\n",
    "            try:\n",
    "                path = chunk[\"file_path\"]\n",
    "                data = await docling_parse(path)\n",
    "                result = {\n",
    "                    \"start_page\": chunk[\"start_page\"],\n",
    "                    \"end_page\": chunk[\"end_page\"],\n",
    "                    \"text\": data,\n",
    "                }\n",
    "                f.write(json.dumps(result) + \"\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Error parsing {chunk}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df9d24f",
   "metadata": {},
   "source": [
    "Then we test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ab604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e7a8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILENAME = os.getenv(\"INPUT_FILENAME\", \"input\")\n",
    "COMPRESSION_OUTPUT_FILENAME = os.getenv(\"COMPRESSION_OUTPUT_FILENAME\", \"compressed_output\")\n",
    "COMPRESSION_QUALITY = os.getenv(\"GHOSTSCRIPT_COMPRESSION_QUALITY\", \"ebook\")\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fad8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = INPUT_FILENAME + \".pdf\"\n",
    "compression_output = COMPRESSION_OUTPUT_FILENAME + str(current_time) + \".pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9192fb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "compress_pdf(input, compression_output, quality=COMPRESSION_QUALITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3e66965",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'split_pdf_into_chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m chunks = \u001b[43msplit_pdf_into_chunks\u001b[49m(compressed_input, \u001b[33m\"\u001b[39m\u001b[33mchunks\u001b[39m\u001b[33m\"\u001b[39m, chunk_size=\u001b[32m20\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'split_pdf_into_chunks' is not defined"
     ]
    }
   ],
   "source": [
    "chunks = split_pdf_into_chunks(compressed_input, \"chunks\", chunk_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5958e77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "await process_all_pages(chunks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
